{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ELbREVTXkgtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "ner_cats = [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\"]\n",
        "not_ner_cats = [\"DATE\"]"
      ],
      "metadata": {
        "id": "7OyE5JvTkvGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''Robert Downey Jr. is an American actor and producer.\n",
        "He is best known for his roles in films such as Iron Man,\n",
        "The Avengers, and Sherlock Holmes. Downey has won several awards\n",
        "for his acting, including two Screen Actors Guild Awards\n",
        "and a Golden Globe Award. He has also been nominated for an Academy Award.'''\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "w7jHVh7Xk9kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = []\n",
        "for ent in doc.ents:\n",
        "  entities.append((ent.text, ent.label_))"
      ],
      "metadata": {
        "id": "CwNiJ6gPlS0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entity, category in entities:\n",
        "  print(f\"{entity}: {category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz7KG6H0lcjf",
        "outputId": "a559181a-7890-495c-b580-f75c16fb98ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Robert Downey Jr.: PERSON\n",
            "American: NORP\n",
            "Iron Man: PERSON\n",
            "Avengers: NORP\n",
            "Sherlock Holmes: PERSON\n",
            "Downey: ORG\n",
            "two: CARDINAL\n",
            "Screen Actors Guild Awards: ORG\n",
            "an Academy Award: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "CBA6-v63hcqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LjIXHgtcx3f",
        "outputId": "f34853e5-6eab-43ba-ffe6-8371b0d0efd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the file containing sentences\n",
        "def load_sentences(file_path):\n",
        "   with open(file_path, 'r') as file:\n",
        "    sentences = file.readlines()\n",
        "   return [sentence.strip() for sentence in sentences]\n",
        "\n",
        "# Preprocess the input sentence\n",
        "def preprocess_sentence(sentence):\n",
        "   tokens = word_tokenize(sentence.lower())\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   tokens = [token for token in tokens if token not in stop_words]\n",
        "   lemmatizer = WordNetLemmatizer()\n",
        "   tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "   return ' '.join(tokens)\n",
        "\n",
        "def get_most_similar_sentence(user_input, sentences):\n",
        "   preprocessed_user_input = preprocess_sentence(user_input)\n",
        "   preprocessed_sentences = [preprocess_sentence(sentence) for sentence in sentences]\n",
        "   vectorizer = TfidfVectorizer()\n",
        "   tfidf_matrix = vectorizer.fit_transform([preprocessed_user_input] + preprocessed_sentences)\n",
        "   similarity_scores = (tfidf_matrix * tfidf_matrix.T).A[0][1:]\n",
        "   most_similar_index = similarity_scores.argmax()\n",
        "   most_similar_sentence = sentences[most_similar_index]\n",
        "   return most_similar_sentence\n",
        "\n",
        "file_path = '/content/drive/MyDrive/OELP_BERT/BERT_SQuad/trial.txt'  # Path to the file containing sentences\n",
        "\n",
        "sentences = load_sentences(file_path)\n",
        "print(type(sentences))\n",
        "print(sentences)\n",
        "\n",
        "user_input = 'hello I am a women'\n",
        "\n",
        "most_similar_sentence = get_most_similar_sentence(user_input, sentences)\n",
        "print('Most similar sentence:', most_similar_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpopjiNHis3A",
        "outputId": "4813434d-6295-4e61-b5a0-8c1830904fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['this is comedy movie.', '', 'this is horror movie.', '', 'hello I am a girl.', '', 'hello I am a boy.']\n",
            "Most similar sentence: hello I am a girl.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmPKOSwOd6PM",
        "outputId": "28f56a7e-cf44-4330-94af-4d83c967766f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-26 08:54:50--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42123633 (40M) [application/json]\n",
            "Saving to: ‘train-v2.0.json’\n",
            "\n",
            "train-v2.0.json     100%[===================>]  40.17M  66.5MB/s    in 0.6s    \n",
            "\n",
            "2024-05-26 08:54:51 (66.5 MB/s) - ‘train-v2.0.json’ saved [42123633/42123633]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json  #importing squad\n",
        "with open('train-v2.0.json', 'rb') as f1:\n",
        "  raw_data = json.load(f1)\n",
        "new_contexts = []\n",
        "question = \"\"\n",
        "sq_contexts = []\n",
        "sq_questions = []\n",
        "sq_answers = []\n",
        "sq_answers_starts = []\n",
        "sq_ids = []\n",
        "count = 0\n",
        "for group in raw_data['data']:\n",
        "  for paragraph in group['paragraphs']:\n",
        "    context = paragraph['context']\n",
        "    for qa in paragraph['qas']:\n",
        "      if(len(qa['answers'])!=0):\n",
        "        question = qa['question']\n",
        "        sq_questions.append(qa['question'])\n",
        "        sq_contexts.append(context)\n",
        "        sq_ids.append(qa['id'])\n",
        "        sq_answers.append(qa['answers'][0]['text'])\n",
        "        sq_answers_starts.append(qa['answers'][0]['answer_start'])\n",
        "      else:\n",
        "        question = qa['question']\n",
        "        sq_questions.append(qa['question'])\n",
        "        sq_contexts.append(context)\n",
        "        sq_ids.append(qa['id'])\n",
        "        sq_answers.append(\"\")\n",
        "        sq_answers_starts.append(-1)"
      ],
      "metadata": {
        "id": "8H36nYeK2XB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sq_ids))"
      ],
      "metadata": {
        "id": "0TX3fDxWbVPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82768f0b-c8db-410d-ec46-95baf8642450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Opening JSON file\n",
        "f = open('/content/drive/MyDrive/OELP_BERT/BERT_SQuad/QGen+Augments.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)\n",
        "\n",
        "# Iterating through the json\n",
        "# list\n",
        "\n",
        "test_questions = []\n",
        "test_sentences = []\n",
        "test_ids = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    test_ids.append(data[i][\"id\"])\n",
        "    test_sentences.append(data[i][\"sentence\"])\n",
        "    test_questions.append(data[i][\"question\"])\n",
        "\n",
        "# Closing file\n",
        "f.close()"
      ],
      "metadata": {
        "id": "jyM3zxF_i0he"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_ids))\n",
        "print(len(test_sentences))\n",
        "print(len(test_questions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn_vtSJJe8kI",
        "outputId": "43f5dfb6-15c6-4d46-8ed8-f6b7293d9be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23293\n",
            "23293\n",
            "23293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "Find all questions with same ids, combine contexts for dataset_context, combine questions into a list, find most similar question, find the entity in that question, check that this is indeed the answer. Then use the question to find the most similar sentence and find the word with the answer in the most similar sentence from the context to the question, find it's position and put answer_start into the dataset."
      ],
      "metadata": {
        "id": "opBoOMvCfKBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ids = []\n",
        "data_contexts = []\n",
        "data_question = []\n",
        "data_answer = []\n",
        "data_answer_start = []"
      ],
      "metadata": {
        "id": "c8-VKRKYfH6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(sq_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxfuStJ6gx03",
        "outputId": "5aae1744-5a21-4501-9775-c199e3eb6e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "err_count = 0\n",
        "found_count = 0"
      ],
      "metadata": {
        "id": "uRTGDiya5lbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_rec = 91\n",
        "idx = test_rec\n",
        "id = test_ids[idx]\n",
        "print(f\"data id: {id}\")\n",
        "print(f\"data q: {test_questions[idx]}\")\n",
        "\n",
        "#get all questions and contexts with the same id\n",
        "indexes = [i for i, sq_id in enumerate(sq_ids) if sq_id == id]\n",
        "temp_contexts = []\n",
        "temp_questions = []\n",
        "temp_ans = \"\"\n",
        "for index in indexes:\n",
        "  temp_contexts.append(sq_contexts[index])\n",
        "  temp_questions.append(sq_questions[index])\n",
        "\n",
        "final_context = \"\"    #making context\n",
        "\n",
        "for ctxt in temp_contexts:\n",
        "  final_context += ctxt\n",
        "final_context = final_context.lower()\n",
        "final_context = final_context.replace(\"é\", \"e\")\n",
        "print(f\"data context: {final_context}\")\n",
        "print(\"all questions\")\n",
        "print(temp_questions)\n",
        "closest_question = get_most_similar_sentence(test_questions[idx], temp_questions)   #finding answer\n",
        "print(f\"Closest_question: {closest_question}\")\n",
        "doc = nlp(closest_question)\n",
        "entities = []\n",
        "for ent in doc.ents:\n",
        "  entities.append((ent.text, ent.label_))\n",
        "\n",
        "cleaned_sentence = re.sub(r'[^\\w\\s]', '', test_questions[idx])\n",
        "q_words = cleaned_sentence.split()\n",
        "\n",
        "for ele in entities:\n",
        "  if ele[0] not in q_words and ele[1] != \"DATE\":\n",
        "    temp_ans = ele[0]\n",
        "    break\n",
        "temp_ans = temp_ans.lower()\n",
        "print(f\"data ans: {temp_ans}\")\n",
        "\n",
        "#getting answer_start\n",
        "if temp_ans == \"\":\n",
        "  print(f\"data ans start: -1\")\n",
        "  data_answer_start.append(-1)\n",
        "else:\n",
        "  sentences = re.split(r'(?<=[.!?]) +', final_context)\n",
        "  for sentence in sentences:\n",
        "    sentence = sentence.lower()\n",
        "  question_doc = nlp(closest_question)\n",
        "  occurrences = []\n",
        "  current_index = 0\n",
        "  best_index = -1\n",
        "  best_similarity = -1\n",
        "  for sentence in sentences:\n",
        "    sentence_start_index = final_context.find(sentence, current_index)\n",
        "    for match in re.finditer(re.escape(temp_ans), sentence):\n",
        "      actual_index = sentence_start_index + match.start()\n",
        "      sentence_doc = nlp(sentence)\n",
        "      similarity = cosine_similarity([question_doc.vector], [sentence_doc.vector])[0][0]\n",
        "      if similarity > best_similarity:\n",
        "          best_similarity = similarity\n",
        "          best_index = actual_index\n",
        "      occurrences.append((actual_index, similarity))\n",
        "    current_index = sentence_start_index + len(sentence)\n",
        "\n",
        "  print(f\"data answer start: {best_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S293mDiGrhsS",
        "outputId": "6a062931-49e4-4741-e13b-4b1a8b4bc632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data id: 57267b57708984140094c796\n",
            "data q: Who opened its first eight retail stores?\n",
            "data context: chain department stores grew rapidly after 1920, and provided competition for the downtown upscale department stores, as well as local department stores in small cities. j. c. penney had four stores in 1908, 312 in 1920, and 1452 in 1930. sears, roebuck & company, a giant mail-order house, opened its first eight retail stores in 1925, and operated 338 by 1930, and 595 by 1940. the chains reached a middle-class audience, that was more interested in value than in upscale fashions. sears was a pioneer in creating department stores that catered to men as well as women, especially with lines of hardware and building materials. it deemphasized the latest fashions in favor of practicality and durability, and allowed customers to select goods without the aid of a clerk. its stores were oriented to motorists – set apart from existing business districts amid residential areas occupied by their target audience; had ample, free, off-street parking; and communicated a clear corporate identity. in the 1930s, the company designed fully air-conditioned, \"windowless\" stores whose layout was driven wholly by merchandising concerns.\n",
            "all questions\n",
            "['How many stores was Sears operating in 1940?']\n",
            "Closest_question: How many stores was Sears operating in 1940?\n",
            "data ans: sears\n",
            "data answer start: 239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_str = \"\""
      ],
      "metadata": {
        "id": "qyoe9utlv58Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, id in enumerate(test_ids):\n",
        "  if idx%1000==0:\n",
        "    print(idx)\n",
        "  data_ids.append(id)\n",
        "  data_question.append(test_questions[idx])\n",
        "  # print(f\"{idx}\")\n",
        "  #file_str += f\"{idx}\\n\"\n",
        "\n",
        "  #get all questions and contexts with the same id\n",
        "  indexes = [i for i, sq_id in enumerate(sq_ids) if sq_id == id]\n",
        "  temp_contexts = []\n",
        "  temp_questions = []\n",
        "  temp_ans = \"\"\n",
        "  for index in indexes:\n",
        "    temp_contexts.append(sq_contexts[index])\n",
        "    temp_questions.append(sq_questions[index])\n",
        "\n",
        "  final_context = \"\"    #making context\n",
        "\n",
        "  for ctxt in temp_contexts:\n",
        "    final_context += ctxt\n",
        "  data_contexts.append(final_context)\n",
        "  # print(\"Context\")\n",
        "  # print(data_contexts[-1])\n",
        "  #file_str += \"Context\\n\"\n",
        "  #file_str += f\"{data_contexts[-1]}\\n\"\n",
        "  # print(\"Question\")\n",
        "  # print(data_question[-1])\n",
        "  #file_str += \"Question\\n\"\n",
        "  #file_str += f\"{data_question[-1]}\\n\"\n",
        "  closest_question = get_most_similar_sentence(test_questions[idx], temp_questions)   #finding answer\n",
        "  doc = nlp(closest_question)\n",
        "  entities = []\n",
        "  for ent in doc.ents:\n",
        "    entities.append((ent.text, ent.label_))\n",
        "\n",
        "  cleaned_sentence = re.sub(r'[^\\w\\s]', '', test_questions[idx])\n",
        "  q_words = cleaned_sentence.split()\n",
        "\n",
        "  for ele in entities:\n",
        "    if ele[0] not in q_words and ele[1] != \"DATE\":\n",
        "      temp_ans = ele[0]\n",
        "      break\n",
        "  data_answer.append(temp_ans)\n",
        "\n",
        "  #getting answer_start\n",
        "  if temp_ans == \"\":\n",
        "    data_answer_start.append(-1)\n",
        "    err_count += 1\n",
        "  else:\n",
        "    sentences = re.split(r'(?<=[.!?]) +', final_context)\n",
        "    for sentence in sentences:\n",
        "      sentence = sentence.lower()\n",
        "    question_doc = nlp(closest_question)\n",
        "    occurrences = []\n",
        "    current_index = 0\n",
        "    best_index = -1\n",
        "    best_similarity = -1\n",
        "    for sentence in sentences:\n",
        "      sentence_start_index = final_context.find(sentence, current_index)\n",
        "      for match in re.finditer(re.escape(temp_ans), sentence):\n",
        "        actual_index = sentence_start_index + match.start()\n",
        "        sentence_doc = nlp(sentence)\n",
        "        similarity = cosine_similarity([question_doc.vector], [sentence_doc.vector])[0][0]\n",
        "        if similarity > best_similarity:\n",
        "            best_similarity = similarity\n",
        "            best_index = actual_index\n",
        "        occurrences.append((actual_index, similarity))\n",
        "      current_index = sentence_start_index + len(sentence)\n",
        "    data_answer_start.append(best_index)\n",
        "    if(best_index==-1):\n",
        "      err_count += 1\n",
        "    else:\n",
        "      found_count += 1\n",
        "\n",
        "  # for entity, category in entities:\n",
        "  #   print(f\"{entity}: {category}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcJu9NDNgC3v",
        "outputId": "b18775aa-7013-4789-b13d-8e744f00f7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/OELP_BERT/BERT_SQuad/Context_question_w_id.txt\", \"w\") as wf:\n",
        "  wf.write(file_str)"
      ],
      "metadata": {
        "id": "6JLxc8SIwPII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_test = \"New York City's most important economic sector lies in its role as the headquarters for the U.S.financial industry, metonymously known as Wall Street. The city's \""
      ],
      "metadata": {
        "id": "37CLaI766miq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(txt_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyPRUdPd6rvt",
        "outputId": "d0acba0d-cb91-4310-c9f3-ea6970b3b626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(err_count)\n",
        "print(found_count)"
      ],
      "metadata": {
        "id": "vnzJw_2k53mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78816589-639c-4c36-c5f8-9aa9b6aeae1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10121\n",
            "13172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stripped_question(question):\n",
        "  question = question.replace(\"\\\"\", \"\")\n",
        "  question = question.replace(\"\\n\", \"\")\n",
        "  question = question.replace(\"\\\\\", \"\\\\\\\\\")\n",
        "  return question"
      ],
      "metadata": {
        "id": "R9Mcx8mAFR8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_str = \"\"\n",
        "file_str += \"[\\n\"\n",
        "for i in range(len(data_contexts)):\n",
        "  file_str += \"{\\n\"\n",
        "  file_str += f\"\\\"id\\\" : \\\"{data_ids[i]}\\\",\\n\"\n",
        "  file_str += f\"\\\"SIno\\\" : {i},\\n\"\n",
        "  file_str += f\"\\\"sentence\\\" : \\\"{stripped_question(data_contexts[i])}\\\",\\n\"\n",
        "  file_str += f\"\\\"question\\\" : \\\"{stripped_question(data_question[i])}\\\",\\n\"\n",
        "  file_str += f\"\\\"answer\\\" : \\\"{stripped_question(data_answer[i])}\\\",\\n\"\n",
        "  file_str += f\"\\\"answer_start\\\" : {data_answer_start[i]},\\n\"\n",
        "  file_str += f\"\\\"answer_end\\\" : 0,\\n\"\n",
        "  if(data_answer_start[i]==-1 or data_answer[i]==\"\"):\n",
        "    file_str += \"\\\"is_impossible\\\": true\\n\"\n",
        "  else:\n",
        "    file_str += \"\\\"is_impossible\\\": false\\n\"\n",
        "  if i!= len(data_contexts)-1:\n",
        "    file_str += \"},\\n\"\n",
        "  else:\n",
        "    file_str += \"}\\n\"\n",
        "file_str += \"]\\n\""
      ],
      "metadata": {
        "id": "QauYJLvPFmba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/OELP_BERT/BERT_SQuad/20k_w_indexes.json\", \"w\") as writefile:\n",
        "  writefile.write(file_str)"
      ],
      "metadata": {
        "id": "vPgLJuf5GJDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DONE HERE"
      ],
      "metadata": {
        "id": "vpafGUbwGMrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNfbxIr3dULk",
        "outputId": "d3d1e2a5-88d5-45e6-8781-757fb75cbe52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "HYXq3GmYdY5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ''\n",
        "\n",
        "def find_exact_answer(context, question):\n",
        "    # Construct the prompt\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer the question based on the context with an exact string from the context.\"\n",
        "\n",
        "    # Call the OpenAI API\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",  # You can switch to \"gpt-4\" if available\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Extract the response text\n",
        "    answer = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "    # Find the exact match in the context\n",
        "    if answer in context:\n",
        "        return answer\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "context = \"\"\"Python is a high-level, interpreted programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected.\"\"\"\n",
        "question = \"What does Python's design philosophy emphasize?\"\n",
        "\n",
        "answer = find_exact_answer(context, question)\n",
        "print(f\"Answer: {answer}\")\n",
        "\n",
        "# Check if the answer is part of the context\n",
        "if answer:\n",
        "    print(f\"The answer '{answer}' is found in the context.\")\n",
        "else:\n",
        "    print(\"The answer was not found in the context.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "OJH6E9xydpu7",
        "outputId": "b870328f-752d-4b9b-c7d6-f1d42487f7d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6f4384b03600>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What does Python's design philosophy emphasize?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_exact_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Answer: {answer}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-6f4384b03600>\u001b[0m in \u001b[0;36mfind_exact_answer\u001b[0;34m(context, question)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Call the OpenAI API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# You can switch to \"gpt-4\" if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def find_best_answer_location(paragraph, question, answer):\n",
        "    # Split paragraph into sentences\n",
        "    sentences = re.split(r'(?<=[.!?]) +', paragraph)\n",
        "\n",
        "    # Process the question with spaCy\n",
        "    question_doc = nlp(question)\n",
        "\n",
        "    occurrences = []\n",
        "    current_index = 0\n",
        "    best_index = -1\n",
        "    best_similarity = -1\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_start_index = paragraph.find(sentence, current_index)\n",
        "\n",
        "        # Find all occurrences of the answer in the current sentence\n",
        "        for match in re.finditer(re.escape(answer), sentence):\n",
        "            # Calculate the actual index in the paragraph\n",
        "            actual_index = sentence_start_index + match.start()\n",
        "\n",
        "            # Process the sentence with spaCy\n",
        "            sentence_doc = nlp(sentence)\n",
        "\n",
        "            # Calculate similarity between the question and the sentence\n",
        "            similarity = cosine_similarity(\n",
        "                [question_doc.vector],\n",
        "                [sentence_doc.vector]\n",
        "            )[0][0]\n",
        "\n",
        "            if similarity > best_similarity:\n",
        "                best_similarity = similarity\n",
        "                best_index = actual_index\n",
        "\n",
        "            occurrences.append((actual_index, similarity))\n",
        "\n",
        "        current_index = sentence_start_index + len(sentence)\n",
        "\n",
        "    return best_index, best_similarity, occurrences\n",
        "\n",
        "# Example usage\n",
        "paragraph = \"This is a sample paragraph. The answer is here. This is another sentence with the answer. Finally, the answer is here again.\"\n",
        "question = \"Where is the answer?\"\n",
        "answer = \"the answer\"\n",
        "\n",
        "best_index, best_similarity, occurrences = find_best_answer_location(paragraph, question, answer)\n",
        "print(f\"Best index: {best_index}, Best similarity: {best_similarity}\")\n",
        "print(\"All occurrences with similarities:\", occurrences)\n"
      ],
      "metadata": {
        "id": "lZ2F7i65qwLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(len(test_ids)):\n",
        "  try:\n",
        "    idk_temp_id = sq_ids.index(test_ids[i])\n",
        "    test_questions.append(sq_questions[i])   #ISSUE\n",
        "    test_contexts.append(sq_contexts[i])\n",
        "    test_answers.append(sq_answers[i])\n",
        "    test_answer_starts.append(sq_answers_starts[i])\n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "4mWl63i8eiJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLS-tnpcg3K6",
        "outputId": "54a69531-ca44-49f7-9f23-09c3c55b77df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15695"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-2RDDzDcGeW",
        "outputId": "e18beff1-7eb4-4758-86f6-e383e8decc4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['this is comedy movie.', '', 'this is horror movie.', '', 'hello I am a girl.', '', 'hello I am a boy.']\n",
            "Most similar sentence: hello I am a girl.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fc65VIDCgJwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/OELP_BERT/BERT_SQuad/final_15k.json', 'w') as f:\n",
        "  f.write(\"[\\n\")\n",
        "  for i in range(len(test_ids)):\n",
        "    f.write(\"{\\n\")\n",
        "    f.write(f\"\\\"id\\\" : \\\"{test_ids[i]}\\\",\\n\")\n",
        "    f.write(f\"\\\"sentence\\\" : \\\"{stripped_question(test_contexts[i])}\\\",\\n\")\n",
        "    f.write(f\"\\\"question\\\" : \\\"{stripped_question(test_questions[i])}\\\",\\n\")\n",
        "    f.write(f\"\\\"answer\\\" : \\\"{stripped_question(test_answers[i])}\\\",\\n\")\n",
        "    f.write(f\"\\\"answer_start\\\" : {test_answer_starts[i]},\\n\")\n",
        "    f.write(f\"\\\"answer_end\\\" : 0,\\n\")\n",
        "\n",
        "    if(test_answer_starts[i]==\"-1\"):\n",
        "      f.write(\"\\\"is_impossible\\\": true\\n\")\n",
        "    else:\n",
        "      f.write(\"\\\"is_impossible\\\": false\\n\")\n",
        "\n",
        "    if(i!=len(test_ids)-1):\n",
        "      f.write(\"},\\n\")\n",
        "    else:\n",
        "      f.write(\"}\\n\")\n",
        "  f.write(\"]\")"
      ],
      "metadata": {
        "id": "bfghh3o9VvcF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}